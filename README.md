# ğŸ§  IntroducciÃ³n y MetodologÃ­a General

## ğŸ¯ Objetivo del Proyecto

El objetivo de esta soluciÃ³n es predecir si un cliente en mora aceptarÃ¡ una opciÃ³n de pago preaprobada en el siguiente mes, representado por la variable binaria `var_rpta_alt`.

La mÃ©trica de evaluaciÃ³n es el **F1 Score** sobre una muestra fuera de tiempo (enero de 2024).

## ğŸ” MetodologÃ­a General

Se implementÃ³ una soluciÃ³n **End-to-End (E2E)** que incluye:

- IngenierÃ­a de datos
- AnÃ¡lisis exploratorio (EDA)
- SelecciÃ³n de caracterÃ­sticas
- Entrenamiento y validaciÃ³n de modelos
- Despliegue vÃ­a API e interfaz web

Todo el proceso fue desarrollado bajo buenas prÃ¡cticas de MLOps, y se documentÃ³ con MLflow para trazabilidad.

## ğŸ“‚ Fuentes de Datos Utilizadas

Para evitar fuga de informaciÃ³n, solo se usaron archivos **sin datos derivados de la aceptaciÃ³n de opciones de pago**. Estos fueron:

- `trtest`: Base principal con la variable objetivo. En donde solo se extrajeron las columnas: nit_enmascarado, num_oblig_orig_enmascarado, num_oblig_enmascarado
- `hist_scores`: Probabilidades histÃ³ricas (propensiÃ³n, autocura, alerta)
- `clientes`: Datos mensuales del cliente
- `pagos`: Historial de pagos

Se filtrÃ³ la Ãºltima fecha por cliente (`fecha_corte`) [por almacenamiento y capacidades de computo] y se integraron usando `nit_enmascarado` como llave. El conjunto final se utilizÃ³ tanto para entrenamiento como inferencia.

---

# ğŸ› ï¸ IngenierÃ­a de Datos

Se construyÃ³ una clase personalizada llamada [`FeatureSelector`](src\data_engineer.py), que automatiza el tratamiento y preparaciÃ³n de datos previo al modelado y para la inferencia. Este pipeline permite estandarizar el flujo tanto en entrenamiento como en inferencia.

### ğŸ”„ Principales transformaciones aplicadas

- **ImputaciÃ³n automÃ¡tica**:
  - Nulos numÃ©ricos: mediana.
  - Nulos categÃ³ricos: valor mÃ¡s frecuente.

- **TransformaciÃ³n de variables numÃ©ricas**:
  - `log1p` en variables con alta asimetrÃ­a.
  - WinsorizaciÃ³n entre percentiles 1% y 99% para reducir outliers extremos.

- **Escalamiento y codificaciÃ³n**:
  - NumÃ©ricas: `StandardScaler`.
  - CategÃ³ricas (baja cardinalidad): `OneHotEncoding` con `drop='first'`.

- **ReconstrucciÃ³n final del dataset**:
  - Uso de `features.csv` para alinear columnas con las seleccionadas por Lasso.
  - AÃ±ade ceros en columnas faltantes para evitar errores en inferencia.
  - Devuelve un DataFrame con `id`, `var_rpta_alt` y variables preprocesadas.

Este proceso asegura que todos los datos pasen por un tratamiento uniforme, evitando fugas, problemas de escala o inconsistencias entre entrenamiento y despliegue.

# ğŸ“Š AnÃ¡lisis Exploratorio de Datos (EDA)

Durante esta etapa se analizaron las relaciones entre la variable objetivo `var_rpta_alt` y distintas variables categÃ³ricas y numÃ©ricas. Se aplicaron pruebas de **Chi-cuadrado** para evaluar dependencia entre variables categÃ³ricas y se usaron grÃ¡ficos para visualizar patrones en las variables numÃ©ricas.

---

## ğŸ¯ Variable Objetivo

- `var_rpta_alt` estÃ¡ **relativamente balanceada**:
  - Clase 0: 53.2%
  - Clase 1: 46.8%

Esto permite un modelado robusto sin necesidad de tÃ©cnicas complejas de balanceo.

![DistribuciÃ³n objetivo](./images/v_objetivo.png)

---

## ğŸ” Variables CategÃ³ricas Relevantes

Se aplicÃ³ la **prueba ChiÂ²** para identificar asociaciÃ³n significativa con la variable objetivo (`p < 0.05`).

- **`banca`**  
  Diferencias marcadas entre â€œPersonasâ€, â€œIndependientesâ€ y â€œPymeâ€.  
  ![banca](./images/distri_banca_clase.png)

- **`rango_mora`**  
  Clientes con mora de `1-30 dÃ­as` tienen mayor aceptaciÃ³n.  
  ![rango mora](./images/distri_rango_mora.png)

- **`alternativa_aplicada_agr`**  
  Influye fuertemente; destaca `SIN_INFO` con alta clase 0.  
  ![alternativa aplicada](./images/distri_alter_apli.png)

- **`cant_promesas_cumplidas_binario`**  
  MÃ¡s aceptaciÃ³n en quienes ya cumplieron promesas.  
  ![promesas cumplidas](./images/cant_prome_cumpl.png)

- **`cant_gestiones_binario`**  
  Mayor aceptaciÃ³n con al menos una gestiÃ³n previa.  
  ![gestiones](./images/cant_gestiones.png)

- **`segmento`**  
  Patrones distintos segÃºn segmento; â€œPersonalâ€ domina en volumen.  
  ![segmento](./images/segmento.png)

- **`marca_alternativa`**  
  Alta aceptaciÃ³n cuando el cliente ya ha aceptado antes  
  âš ï¸ **Posible data leakage**  
  ![marca alternativa](./images/marca_alter.png)

---

## ğŸ“ˆ Variables NumÃ©ricas Relevantes

- **`porc_pago_mes`**  
  A mayor proporciÃ³n pagada, mayor aceptaciÃ³n.  
  ![porc pago mes](./images/porc_pago_mes.png)

- **`rpc` (realizÃ³ pago completo)**  
  Fuerte predictor positivo  
  âš ï¸ Evaluar posible fuga de informaciÃ³n  
  ![rpc](./images/rpc.png)

- **`endeudamiento`**  
  Menor endeudamiento â†’ mayor propensiÃ³n a aceptar.  
  ![endeudamiento](./images/endeudamiento.png)

- **`dias_mora_fin`**  
  Menos dÃ­as en mora â†’ mayor aceptaciÃ³n (comportamiento esperado).  
  ![dias mora fin](./images/dias_mora_fin.png)

---

## ğŸ”— AnÃ¡lisis Multivariado

Se calculÃ³ la matriz de correlaciÃ³n de Pearson para detectar:

- **Alta colinealidad** entre variables similares:
  - Ej: `valor_cuota_mes` â‰ˆ `valor_cuota_mes_pago`
  - Ej: `saldo_capital` â‰ˆ `vr_obligacion`
- **Relaciones negativas destacadas**:
  - `max_mora` â†” `prob_propension` (r â‰ˆ -0.83)

Estas correlaciones guiaron la eliminaciÃ³n de variables redundantes en la selecciÃ³n de caracterÃ­sticas.

![correlaciones](./images/corr_matriz.png)

---

> âœ… Este anÃ¡lisis permitiÃ³ identificar las variables mÃ¡s informativas y detectar posibles riesgos de **fuga de informaciÃ³n**, asegurando un set de variables confiables para el modelado.


# ğŸ§ª SelecciÃ³n de CaracterÃ­sticas

El objetivo de esta etapa fue reducir la dimensionalidad y quedarnos Ãºnicamente con las variables mÃ¡s informativas, minimizando redundancia y riesgo de sobreajuste.

---

## 1ï¸âƒ£ Preprocesamiento y CodificaciÃ³n

- Se identificaron variables numÃ©ricas y categÃ³ricas de baja cardinalidad (â‰¤ 30 categorÃ­as).
- Se aplicaron pipelines de transformaciÃ³n con:
  - **ImputaciÃ³n** (`mean` para numÃ©ricas, `most_frequent` para categÃ³ricas)
  - **EstandarizaciÃ³n** (`StandardScaler`)
  - **CodificaciÃ³n** (`OneHotEncoder` con `drop='first'`)

ğŸ”¹ X shape: (55780, 52)

ğŸ”¹ X_preprocessed shape: (55780, 145)

---

## 2ï¸âƒ£ EliminaciÃ³n de Colinealidad
- Se calculÃ³ la matriz de correlaciÃ³n absoluta.
- Se eliminaron variables con correlaciÃ³n > 0.90 respecto a otras.
- Variables eliminadas: 14

 ğŸ“Œ Variables eliminadas por colinealidad: 14

âœ… Shape despuÃ©s de quitar colinealidad: (55780, 131)

---

## 3ï¸âƒ£ SelecciÃ³n con Lasso (L1)
- Se entrenÃ³ un modelo LogisticRegression con penalizaciÃ³n L1.

- Se conservaron Ãºnicamente las variables con coeficientes distintos de cero.

- Variables seleccionadas: 113 de 131

âœ… Variables seleccionadas por Lasso: 113 de 131

---

## ğŸ“Š Ranking de Importancia
Se ordenaron los coeficientes absolutos del modelo Lasso, y se graficaron las Top 20 variables mÃ¡s relevantes.

Estas variables reflejan fuerte influencia sobre la predicciÃ³n de aceptaciÃ³n (var_rpta_alt).


âœ… Las variables seleccionadas fueron exportadas a [`features.csv`](./data/procesed/features.csv) para ser reutilizadas en la etapa de entrenamiento e inferencia.

![Top variables seleccionadas por Lasso](./images/features.png)

### InterpretaciÃ³n de las Top 5 variables (Lasso)
1. **subsegm_PREF_CONCILIACION**: Clientes de este subsegmento tienen baja probabilidad de aceptaciÃ³n.

2. **producto_SOBREGIRO**: Obligaciones tipo sobregiro se asocian con no aceptaciÃ³n.

3. **producto_LIBRANZA**: Las libranzas presentan menor disposiciÃ³n a aceptar alternativas.

4. **marca_pago_NO_PAGO**: Historial de no pago reduce fuertemente la propensiÃ³n.

5. **ctrl_terc_EXCLIENTE**: Exclientes tienen baja probabilidad de aceptar opciones de pago.


## ğŸ¤– SelecciÃ³n de Modelo (Model Selection)
Se evaluaron mÃºltiples algoritmos de clasificaciÃ³n para predecir la variable binaria `var_rpta_alt` (acepta o no una opciÃ³n de pago). El objetivo fue encontrar el modelo con mejor desempeÃ±o, validado tanto por mÃ©tricas estÃ¡ndar como por estabilidad en validaciÃ³n cruzada.

## ğŸ§ª Modelos evaluados
Se compararon cuatro modelos clÃ¡sicos de clasificaciÃ³n utilizando un split 60/40 y validaciÃ³n cruzada (cv=5):

- LogisticRegression
- KNeighborsClassifier
- DecisionTreeClassifier
- RandomForestClassifier

El proceso se automatizÃ³ e integrÃ³ con MLflow, permitiendo trazabilidad completa de cada experimento: parÃ¡metros, mÃ©tricas y artefactos del modelo.

## âš™ï¸ Pipeline de evaluaciÃ³n
- TransformaciÃ³n del dataset usando la clase `FeatureSelector`.
- SeparaciÃ³n en conjuntos de entrenamiento (60%) y prueba (40%).
- Entrenamiento + validaciÃ³n cruzada para cada modelo.
- CÃ¡lculo de mÃ©tricas: F1 Score, Precision, Recall, Matriz de ConfusiÃ³n, y Classification Report.
- Registro de resultados en MLflow.

## ğŸ“Š Resultados

| Modelo                 | F1 Score (test) | Precision | Recall  | F1 CV Mean Â± Std |
|------------------------|-----------------|-----------|---------|------------------|
| Logistic Regression     | 0.5816          | 0.7297    | 0.7297  | 0.5799 Â± 0.0062  |
| K-Nearest Neighbors     | 0.5306          | 0.6976    | 0.6976  | 0.5228 Â± 0.0054  |
| Decision Tree           | 0.5482          | 0.6874    | 0.6874  | 0.5488 Â± 0.0058  |
| Random Forest           | 0.6193          | 0.7610    | 0.7610  | 0.6116 Â± 0.0101  |

âœ… El mejor modelo fue **Random Forest**, con un F1 score de 0.6193, superando a los demÃ¡s en todas las mÃ©tricas clave.

## ğŸ› ï¸ Ajuste de HiperparÃ¡metros
Se realizÃ³ una bÃºsqueda exhaustiva con GridSearchCV sobre los siguientes hiperparÃ¡metros del modelo Random Forest:

- `n_estimators`: [100, 200]
- `max_depth`: [5, 10]
- `min_samples_split`: [2, 5]
- `min_samples_leaf`: [1, 2, 5]
- `max_features`: ['sqrt', 'log2']

Se evaluaron 72 combinaciones, utilizando validaciÃ³n cruzada con `scoring='f1'`.

### âœ… Mejor configuraciÃ³n encontrada
- `max_depth`: 10  
- `max_features`: 'sqrt'  
- `min_samples_leaf`: 2  
- `min_samples_split`: 2  
- `n_estimators`: 200  

## ğŸ“ˆ MÃ©tricas del mejor modelo (RandomForest + Hyperparameter Optimization (HPO))

| MÃ©trica     | Valor  |
|-------------|--------|
| F1 Score    | 0.5835 |
| Precision   | 0.7116 |
| Recall      | 0.4944 |
| Accuracy    | 0.76   |



> ğŸ“ Todos los modelos, mÃ©tricas y artefactos fueron registrados y gestionados mediante **MLflow**, lo que facilita su trazabilidad y posterior despliegue.

## ğŸš€ Despliegue e Inferencia

El modelo entrenado fue empaquetado y desplegado usando una arquitectura sencilla pero efectiva que permite su **consumo vÃ­a API REST** y a travÃ©s de una **interfaz web interactiva con Streamlit**.


## ğŸ§  API de PredicciÃ³n (FastAPI)

Se construyÃ³ una API en **FastAPI** que permite generar predicciones cargando un archivo `.csv` o especificando una ruta local. La API expone el siguiente endpoint:

### POST `/predecir`

Permite:

- Seleccionar el modelo deseado (por nombre).
- Cargar un archivo OOT directamente o especificar la ruta en disco.
- Retornar automÃ¡ticamente un archivo `submission.csv` con los resultados.

La lÃ³gica estÃ¡ encapsulada en una clase `Predictor`, que recibe el modelo, ejecuta el preprocesamiento y guarda el archivo listo para subir como sumisiÃ³n.

---

## ğŸ–¥ï¸ Interfaz de Usuario (Streamlit)

Se creÃ³ una interfaz web con **Streamlit** que permite:

- Subir un archivo `.csv`.
- Seleccionar el modelo entrenado.
- Enviar la solicitud a la API.
- Descargar el archivo con los resultados.

Esto permite que cualquier persona del negocio o del equipo de riesgo pueda generar predicciones sin necesidad de conocimientos tÃ©cnicos.

---

## ğŸ³ ContenerizaciÃ³n con Docker

Para facilitar la ejecuciÃ³n del sistema completo en cualquier entorno, se construyeron dos imÃ¡genes Docker:

### API (FastAPI): definida en `Dockerfile`

- Expone el puerto `8000`.
- Ejecuta la API con `uvicorn`.

### App (Streamlit): definida en `dockerfile.streamlit`

- Expone el puerto `8501`.
- Permite levantar la interfaz grÃ¡fica vÃ­a navegador.

Ambos servicios se pueden orquestar fÃ¡cilmente usando `docker-compose`, facilitando el despliegue local o en servidores remotos.

# â–¶ï¸ EjecuciÃ³n Local

A continuaciÃ³n se describen los pasos para levantar el sistema de predicciÃ³n de forma local, sin usar Docker.

---

### ğŸ”¹ 1ï¸âƒ£ Levantar sin Docker

1. **Clonar el repositorio**:

*git clone https://github.com/juanguzman2/prueba_tecnica_MELI.git*

2. **Crear un entorno virtual y activar**:

Cerciorarse de que se estÃ¡ en la carpeta base:

- *cd juan_guzman_prueba*

Crear el entorno virtual:

- *python -m venv env*

Activar el entorno virtual:

- *.\env\Scripts\activate*   (En Windows)

3. **Instalar dependencias**

*pip install -r requirements.txt*

4. **Ejecutar la API con FastAPI**

*uvicorn src.api:app --host 127.0.0.1 --port 8000 --reload*

5. **Ejecutar la interfaz en Streamlit**

Abrir otra terminal y asegurarse de estar en la carpeta principal:

- *cd juan_guzman_prueba*

Activar el entorno virtual:

- *.\env\Scripts\activate*   (En Windows)

Levantar la app:

- *streamlit run webapp/app.py*

---

* Acceder a la API: http://localhost:8000/
* Acceder a la documentaciÃ³n de la API: http://localhost:8000/docs
* Acceder a la interfaz grÃ¡fica (Streamlit): http://localhost:8501/

# âœ… Conclusiones Generales del Proyecto

1. Se desarrollÃ³ una soluciÃ³n analÃ­tica E2E robusta para predecir la aceptaciÃ³n de opciones de pago por parte de clientes en mora, alineada con las necesidades estratÃ©gicas de Bancolombia.

2. El modelo final, basado en Random Forest y ajustado con GridSearchCV, obtuvo mÃ©tricas sÃ³lidas (F1: 0.58, Precision: 0.71) sobre un conjunto de prueba realista (OOT enero 2024), y fue registrado con trazabilidad completa en MLflow.

3. Se aplicaron buenas prÃ¡cticas de ingenierÃ­a de datos, como imputaciÃ³n, codificaciÃ³n, escalamiento, eliminaciÃ³n de colinealidad y selecciÃ³n con Lasso, resultando en un conjunto de 113 variables relevantes.

4. La soluciÃ³n es modular y productizable, con:

  *  API REST en FastAPI para consumo automatizado.

  *  Interfaz Streamlit para usuarios del negocio.

  * ContenerizaciÃ³n completa con Docker.

5. El modelo es interpretativo y arroja insights valiosos sobre segmentos, productos y comportamientos que influyen en la propensiÃ³n a aceptar opciones de pago, facilitando decisiones tÃ¡cticas de cobranza.

6. El diseÃ±o permite escalar hacia arquitecturas mÃ¡s complejas de MLOps, incluyendo monitoreo, versionado de modelos y automatizaciÃ³n del retraining.

7. `Se recomienda que, bajo la arquitectura actual de Bancolombia, todo el almacenamiento de datos se realice a travÃ©s de la Landing Zone (LZ), asegurando gobernabilidad, trazabilidad y cumplimiento con los lineamientos institucionales de datos.`

8. `Para el control de versiones y gestiÃ³n colaborativa del cÃ³digo y modelos, se sugiere utilizar repositorios en Azure DevOps, lo que permite auditar cambios, integrar flujos CI/CD y escalar la soluciÃ³n dentro de prÃ¡cticas de MLOps.`


# Propuesta Funcional: Interfaz Conversacional para el Proyecto de PredicciÃ³n de AceptaciÃ³n de Opciones de Pago â€“ Bancolombia

## ğŸ§  Objetivo de la Interfaz Conversacional

Permitir que usuarios no tÃ©cnicos interactÃºen de forma natural con el sistema analÃ­tico, accediendo a insights del modelo de predicciÃ³n y la informaciÃ³n histÃ³rica del cliente, sin necesidad de escribir consultas complejas o manejar bases de datos directamente.

---

## ğŸ” Â¿QuÃ© permitirÃ­a hacer esta interfaz?

### 1. Consultar Predicciones Individuales
**Ejemplo:**  
> â€œÂ¿QuÃ© probabilidad tiene Juan PÃ©rez de aceptar una opciÃ³n de pago este mes?â€

**Funcionalidad:**  
La interfaz devuelve la probabilidad calculada por el modelo, el score, y las variables mÃ¡s influyentes en su predicciÃ³n (explainability).

---

### 2. Explorar Segmentos de Clientes
**Ejemplo:**  
> â€œMuÃ©strame los clientes con alta probabilidad de aceptar el plan de pago pero que aÃºn no han sido contactados.â€

**Funcionalidad:**  
Devuelve un segmento filtrado dinÃ¡micamente con insights accionables.

---

### 3. Revisar la Historia del Cliente
**Ejemplo:**  
> â€œÂ¿QuÃ© pagos ha hecho el cliente 12345 en los Ãºltimos 6 meses?â€

**Funcionalidad:**  
Consulta cruzada con los datos de pagos histÃ³ricos, cuotas, comportamiento.

---

### 4. Simular Escenarios
**Ejemplo:**  
> â€œÂ¿QuÃ© pasarÃ­a si al cliente X se le ofrece una opciÃ³n con menor cuota?â€

**Funcionalidad:**  
Permite simular el cambio de algunas variables y obtener una nueva predicciÃ³n (tipo â€œwhat-ifâ€).


## ğŸ’¡ Casos de uso especÃ­ficos en contexto Bancolombia

- **Gestores de cobranza**: pueden consultar el perfil de un cliente y recibir recomendaciones sobre cÃ³mo abordarlo segÃºn la probabilidad de aceptaciÃ³n y su historial.
- **Ãrea de inteligencia comercial**: puede explorar segmentos de clientes para definir campaÃ±as proactivas.
- **Analistas de riesgo**: pueden evaluar el impacto de nuevas variables.

---